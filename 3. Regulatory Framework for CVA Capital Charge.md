# REGULATORY FRAMEWORK FOR CVA CAPITAL CHARGE

## 3.1 Rationale for the CVA Capital Charge

The Credit Valuation Adjustment (CVA) capital charge arose from a clear gap revealed during the 2007–2009 crisis: banks incurred large mark-to-market losses because counterparties’ credit spreads widened, even when no default occurred. Pre-crisis capital rules focused on default probability and loss-given-default, leaving spread-driven valuation effects outside Pillar 1 (Gregory, 2015; Brigo & Morini, 2010). Institutions such as Lehman Brothers’ trading partners and AIG’s derivative counterparties recorded sizeable losses when expected future cash flows were discounted with higher counterparty credit spreads. Those valuation hits flowed through earnings, but they were not backed by regulatory capital, creating a disconnect between risk measurement and loss-absorbing capacity (BCBS, 2011; Green, Kenyon, & Dennis, 2014).

Regulators drew three lessons. First, counterparty credit risk is not purely a binary default event; it has a market component that moves with credit spreads. Second, this market component can be systemic, because spread widening typically coincides with liquidity stress, margin calls, and deleveraging. Third, incentives matter: if hedging credit-spread risk does not reduce capital, dealers may under-hedge and rely on accounting P&L to absorb volatility (BCBS, 2015; Duffie & Singleton, 2012).

The CVA capital charge addresses these issues by requiring banks to hold capital against potential changes in portfolio value attributable to counterparty credit quality before default. In the standardized form, the charge scales with three intuitive drivers: exposure (EAD), counterparty credit quality via risk weights, and effective maturity, aggregated across obligors. This mapping is deliberately simple, increasing comparability across firms while preserving a first-order sensitivity to spread risk (BCBS, 2011). For banks with model approval, earlier Basel III versions allowed a CVA-VaR alternative linked to simulated spread shocks, strengthening alignment between prudential capital and internal price/hedge systems (BCBS, 2015).

From a macro-prudential perspective, capitalizing CVA is meant to dampen procyclicality. During stress, wider credit spreads reduce valuations and raise collateral needs; without a capital buffer, institutions may be forced to sell assets or curtail market-making, amplifying volatility. A dedicated CVA buffer helps pre-position loss-absorbing resources for these spread shocks and reduces the likelihood that mark-to-market losses immediately translate into solvency or liquidity pressure (Stulz, 2009; BCBS, 2011). At the same time, Basel recognizes eligible hedges—typically single-name or index CDS—so that genuine risk reduction can lower both P&L volatility and capital requirements (BCBS, 2015).

Economically, the CVA charge also internalizes externalities in bilateral OTC markets. When a bank prices a client trade tightly but leaves spread risk unhedged, it effectively writes a short position in the client’s credit quality. If spreads widen system-wide, unhedged losses can spill over to creditors and taxpayers. By embedding CVA into capital, the framework aligns private pricing incentives with social costs, encouraging collateralization, netting under robust legal agreements, and the migration of standardized products to central clearing where feasible (BCBS & IOSCO, 2015; Gregory, 2015).

In summary, the rationale for the CVA capital charge is to close the pre-crisis gap between economic risk and prudential resources, reduce procyclical feedback loops, and improve incentives to hedge and collateralize counterparty risk. 

## 3.2 Basel II and the Emergence of Counterparty-Risk Regulation

The treatment of counterparty credit risk (CCR) first entered the international regulatory framework through Basel II, which was finalized in 2004 and implemented in the years leading up to the global financial crisis. Basel II aimed to increase the sensitivity of capital requirements to underlying risk by linking them to banks’ internal ratings-based systems and more granular exposure measures (BCBS, 2005). The framework introduced three regulatory pillars: (1) minimum capital requirements, (2) supervisory review, and (3) market discipline. Counterparty risk was placed under Pillar 1, alongside traditional credit and market risk, representing the first formal attempt to quantify exposures arising from over-the-counter (OTC) derivatives, securities financing transactions, and long-settlement trades.

Under Basel II, banks could calculate the exposure at default (EAD) for derivative positions using one of two primary methods: the Current Exposure Method (CEM) or, with supervisory approval, the Internal Model Method (IMM). Both methods sought to measure the potential loss to the bank should a counterparty default before the transaction’s maturity. The standardized approach relied on the formula:

EAD = α × EEPE

where α = 1.4 served as a regulatory multiplier and EEPE represented the effective expected positive exposure (BCBS, 2005). The concept of expected exposure (EE) captured the average future exposure profile, while EPE and EEPE integrated time-weighted effects and netting benefits within legally enforceable master agreements.

While this design was innovative for its time, Basel II had a critical limitation: it focused exclusively on default-related losses and ignored market-driven changes in counterparty credit quality. The underlying assumption was that credit losses would materialize only if a counterparty actually defaulted. As a result, no capital was held for mark-to-market losses arising from credit-spread widening—a risk that would later prove substantial during the 2008 crisis (Gregory, 2015). The framework also failed to recognize wrong-way risk, where exposure increases when a counterparty’s credit quality deteriorates, thereby compounding potential losses (Pykhtin & Zhu, 2007).

The Basel II approach was also criticized for its reliance on static supervisory add-ons and limited sensitivity to collateralization or portfolio diversification. Under the CEM, exposures were approximated through simple add-on factors tied to notional amounts and asset classes. For instance, interest-rate derivatives were assigned add-ons ranging from 0.5% to 1.5% of notional value, depending on maturity. These coefficients were designed for comparability but were not empirically calibrated to reflect true market volatility. As a result, they tended to overestimate exposure for well-collateralized portfolios and underestimate it for highly volatile or non-linear structures (Hull, 2010). 

Meanwhile, the Internal Model Method offered greater flexibility by allowing banks to use Monte Carlo simulation to model future exposures. However, it required advanced systems, extensive data, and prior supervisory approval, which limited its use to large international institutions. Smaller and mid-sized banks were confined to the standardized methods that lacked true risk sensitivity. Consequently, regulatory capital requirements across banks varied widely, and the same derivative portfolio could produce vastly different EAD figures under different methodologies (EBA, 2016). 

Supervisory reviews and quantitative impact studies in the late 2000s revealed that Basel II underestimated counterparty risk at a systemic level. The framework’s narrow focus on default probability ignored the valuation impact of credit deterioration and collateral dynamics. When credit spreads widened sharply during 2008, institutions incurred large CVA losses that eroded Tier 1 capital, yet these were not reflected in their regulatory capital calculations (BCBS, 2011). This shortcoming underscored the need to extend the capital framework beyond default-only risk to include mark-to-market valuation effects. 

In addition to its risk-measurement weaknesses, Basel II’s three-pillar structure relied heavily on market discipline (Pillar 3) to enforce prudence. Banks were expected to disclose CCR metrics and stress results, allowing investors to reward prudent risk management. However, the crisis revealed that market discipline was ineffective under systemic stress: information asymmetry and opacity in derivatives markets prevented investors from assessing exposures accurately (Jorion, 2010). Supervisory Pillar 2 interventions also proved inconsistent across jurisdictions, leading to fragmented oversight and arbitrage opportunities.

These lessons shaped the evolution toward Basel III. The new framework recognized that a purely default-based capital charge was insufficient in an interconnected financial system dominated by derivatives and collateralized exposures. The 2008 crisis demonstrated that losses could emerge from both credit events and credit-spread volatility. Basel III therefore introduced the CVA capital charge as a new component of counterparty-risk capital, explicitly designed to capture market-driven changes in credit quality (BCBS, 2011). 

Basel II laid the groundwork for measuring counterparty risk but failed to internalize its dynamic and market-sensitive nature. Its dependence on static conversion factors, narrow focus on default, and limited recognition of collateral created a regulatory blind spot that the financial crisis exposed. Basel III’s reforms, particularly the inclusion of CVA risk, directly addressed these deficiencies by expanding the scope of capital requirements from pure default risk to a more comprehensive view of counterparty exposure.


## 3.3 Basel III Framework for CVA and Counterparty Credit Risk

Basel III, published in 2010 and revised in 2015, represents the most significant regulatory reform in the treatment of counterparty credit risk (CCR). In contrast to Basel II, which focused almost exclusively on default-driven losses, Basel III introduced a new capital requirement to address market-driven valuation losses arising from changes in counterparties’ credit spreads. The inclusion of the Credit Valuation Adjustment (CVA) capital charge marked a conceptual shift from a static, default-only view of credit risk to a dynamic perspective that recognizes credit-spread volatility as a source of capital depletion (BCBS, 2011).

Under Basel III, the total capital requirement for CCR was divided into two complementary components: the Default Risk Capital Charge (DRCC), covering expected and unexpected losses due to actual defaults, and the CVA Risk Capital Charge, covering potential mark-to-market losses from the deterioration in counterparties’ creditworthiness. The total counterparty capital requirement can therefore be represented as:

K<sub>CCR</sub> = K<sub>Default</sub> + K<sub>CVA</sub>  

The CVA capital charge links the fair value of derivatives to the market perception of counterparties’ credit risk. In the standardized version, the charge is calculated using the supervisory formula:

K<sub>CVA</sub> = 2.33 × √h × Σ (w<sub>i</sub> × M<sub>i</sub> × EAD<sub>i</sub>),

where 2.33 corresponds to the 99th percentile of the normal distribution, h is a one-year time horizon, w_i is the risk weight associated with each counterparty’s credit rating, M_i is the effective maturity, and EAD_i represents the exposure at default (BCBS, 2011). This formula captures the main risk drivers of CVA while remaining simple enough to maintain cross-bank comparability. For banks with advanced modeling capabilities, Basel III initially permitted the use of an internal Value-at-Risk (VaR) approach, known as the CVA-VaR model, to simulate the potential volatility of credit spreads and estimate the associated capital charge (BCBS, 2015).

The introduction of CVA within Basel III aligned regulatory capital with fair-value accounting principles already used in derivative valuation. Prior to 2008, CVA was recognized for financial reporting purposes but ignored in regulatory capital calculations, leading to a disconnect between accounting losses and prudential buffers (Green, Kenyon, & Dennis, 2014). By incorporating CVA under Pillar 1, Basel III established a formal bridge between valuation practices and capital adequacy standards, ensuring that both realized defaults and unrealized valuation losses are captured within the prudential framework.

Another defining feature of Basel III was its emphasis on the interaction between CVA and collateralization. The framework recognized that collateral and margining are key determinants of counterparty exposure. In 2015, the BCBS and IOSCO introduced margin requirements for non-centrally cleared derivatives, mandating daily variation and initial margin exchanges between covered entities (BCBS & IOSCO, 2015). These reforms reduced unsecured exposures—the main driver of CVA volatility—and encouraged migration of standardized trades to central clearing counterparties (CCPs). Basel III reinforced this trend by assigning lower risk weights to exposures toward qualifying CCPs (QCCPs) and higher weights to non-centrally cleared trades, thereby linking micro-level incentives to macro-level stability objectives.

While Basel III offered both standardized and model-based options for calculating CVA capital, practical experience revealed several implementation challenges. The standardized approach, though transparent, relied on fixed supervisory parameters that limited risk sensitivity. Conversely, the CVA-VaR model provided greater responsiveness to portfolio characteristics but introduced significant model risk and operational burden. Supervisory benchmarking by the European Banking Authority (EBA, 2016) showed considerable dispersion in CVA-VaR outcomes for identical portfolios, highlighting the difficulty of validating correlation structures and volatility calibrations consistently across institutions (Green & Kenyon, 2015). These challenges ultimately prompted the Basel Committee to reconsider the feasibility of internal CVA models and paved the way for a more standardized approach under Basel IV.

Despite these technical limitations, the introduction of CVA capital achieved several key regulatory objectives. It reduced the capital blind spot that had allowed large mark-to-market losses to erode capital ratios during the financial crisis, and it strengthened incentives for banks to hedge or collateralize their counterparty exposures. The CVA charge also enhanced the resilience of the financial system by internalizing the cost of spread risk within individual institutions, thereby mitigating systemic contagion during periods of market stress (Stulz, 2009; BCBS, 2015).

However, the reform also created new trade-offs. For many banks, especially those with significant client-facing derivatives activity, the CVA capital requirement increased the cost of offering uncollateralized trades. This led some institutions to reduce bilateral activity or transfer risk to less-regulated sectors, raising concerns about market liquidity and regulatory arbitrage (Green & Kenyon, 2015). These side effects underscored the inherent tension between prudential simplicity and economic efficiency—an issue that subsequent Basel IV revisions sought to balance by simplifying calibration while maintaining conservative capital buffers.

Basel III represented a turning point in the regulatory treatment of counterparty credit risk. By incorporating CVA risk into capital requirements, the framework bridged the gap between accounting valuation and prudential oversight, reinforcing both micro-prudential discipline and macro-prudential stability. The experience gained from implementing the Basel III CVA framework provided valuable lessons for the further refinements introduced under Basel IV, which aimed to consolidate these principles into a more standardized and globally consistent approach.

## 3.4 The Current Exposure Method (CEM)

The Current Exposure Method (CEM) was one of the earliest standardized approaches introduced under Basel II for calculating the exposure at default (EAD) associated with derivative transactions. Its purpose was to provide a simple and transparent framework for measuring counterparty credit risk (CCR), allowing banks to quantify potential losses if a counterparty were to default prior to the maturity of the transaction (BCBS, 2005). The method remained in widespread use for more than a decade and served as the regulatory foundation for capital requirements under Basel II and the early Basel III regime.

Under the CEM framework, the exposure at default is defined as the sum of two components: the current exposure, which is the positive mark-to-market value of the contract, and the potential future exposure (PFE), which represents an add-on amount reflecting possible increases in exposure over the remaining life of the transaction. The formula is expressed as:

EAD = max(V, 0) + β × N,

where *V* is the current market value of the derivative, *N* is the notional amount, and β is an add-on factor determined by the asset class and maturity. For interest-rate derivatives, β typically ranges from 0.5% for maturities under one year to 1.5% for maturities exceeding five years (BCBS, 2005).  

CEM’s design was intentionally simple to enhance cross-bank comparability and reduce data requirements. However, this simplicity came at the cost of economic realism. The method assumes that exposures scale linearly with notional amounts and ignores the impact of netting, collateralization, and volatility dynamics. Consequently, CEM tends to overestimate exposures for portfolios subject to frequent margining and underestimate them for long-dated or highly volatile positions (Gregory, 2015; Pykhtin & Zhu, 2007).

Another significant limitation of the CEM is its inability to account for legally enforceable netting agreements. For example, consider a bank holding two offsetting interest-rate swaps with the same counterparty under a master netting agreement. Economically, the exposures may largely cancel each other, but under CEM the add-ons for both trades are summed independently, resulting in overstated exposure and inflated capital requirements. This static treatment fails to capture the true risk-reducing effects of netting and collateral arrangements that have become standard in modern derivatives markets (Hull, 2010).

CEM also fails to reflect wrong-way risk—situations in which exposure increases as the counterparty’s credit quality deteriorates. For example, a commodity swap with an energy producer may exhibit higher exposure precisely when the producer’s financial condition worsens. Because the CEM formula uses fixed add-on factors and ignores correlation between market and credit variables, it provides no mechanism for capturing such tail dependencies (Pykhtin & Zhu, 2007). The omission of wrong-way risk became one of the most serious criticisms of the method following the financial crisis.

From a regulatory perspective, the simplicity of CEM was both its strength and its weakness. On one hand, its transparency allowed supervisors to implement capital rules consistently across jurisdictions. On the other hand, its lack of sensitivity to portfolio composition and collateralization led to widely divergent capital outcomes across institutions with similar risk profiles (EBA, 2016). For example, two banks trading the same notional amounts could report vastly different exposures depending on whether they were using CEM or an approved internal model. This inconsistency undermined comparability and created incentives for regulatory arbitrage, as banks optimized their trading structures to minimize capital rather than economic risk (Gregory, 2015).

The limitations of CEM became increasingly evident as the derivatives market evolved. The growing use of collateral under Credit Support Annexes (CSAs) and the expansion of central clearing rendered the fixed add-on structure outdated. As collateralization became standard practice, many bilateral exposures were substantially reduced in economic terms, but CEM continued to assign high capital charges because it failed to recognize margining frequency or threshold levels (BCBS, 2011). This disconnect between regulation and market practice encouraged the Basel Committee to develop a new, more risk-sensitive approach.

The eventual replacement of CEM by the Standardized Approach for Counterparty Credit Risk (SA-CCR) under Basel III was therefore a direct response to these deficiencies. SA-CCR introduced explicit recognition of netting sets, supervisory deltas, and maturity factors, allowing more accurate differentiation between collateralized and uncollateralized trades. In this sense, CEM served as a transitional framework—useful for promoting consistency in early capital regulation but ultimately too crude for the complexity of modern derivatives portfolios.

The Current Exposure Method represented an important milestone in the regulatory treatment of counterparty risk. Its simplicity facilitated global implementation, but its assumptions about linear exposure growth, absence of netting, and neglect of collateralization limited its accuracy and fairness. The method’s shortcomings became particularly visible during the financial crisis, when static add-ons failed to capture the dynamic nature of exposure and credit deterioration. Basel III’s replacement of CEM with SA-CCR marked a crucial step toward greater risk sensitivity and consistency in measuring counterparty credit risk.

## 3.5 Standardized Approach for Counterparty Credit Risk (SA-CCR)

The Standardized Approach for Counterparty Credit Risk (SA-CCR) was introduced by the Basel Committee in 2014 and became effective from 2017, replacing the long-standing Current Exposure Method (CEM). Its development was a direct response to the deficiencies exposed by the financial crisis, particularly the inability of CEM to account for collateralization, netting, and the true volatility of derivative exposures (BCBS, 2017). The purpose of SA-CCR was not only to provide greater risk sensitivity but also to ensure consistency across jurisdictions and reduce the model variability that had undermined confidence in internal approaches under Basel II and III.

SA-CCR is based on a more nuanced understanding of counterparty exposure dynamics. Instead of applying fixed add-on percentages to notional amounts, the new framework decomposes exposure into two components: the replacement cost (RC) and the potential future exposure (PFE). The exposure at default (EAD) is then defined as:

EAD = α × (RC + PFE),

where α = 1.4 serves as a regulatory multiplier designed to ensure prudence. The replacement cost reflects the current mark-to-market value of the derivative portfolio, adjusted for eligible collateral and netting, while the potential future exposure captures the possible increase in exposure due to market movements over the life of the contract (BCBS, 2017).

Compared with the CEM, the key innovation of SA-CCR lies in how it models PFE. Rather than relying on static add-ons, it employs supervisory factors (SF) calibrated by asset class, tenor, and underlying risk driver. These factors are combined with delta adjustments and maturity corrections that account for the direction and sensitivity of each position. As a result, SA-CCR captures both the diversification effects across hedging sets and the nonlinear risk of derivative portfolios (Gregory, 2015). For example, interest-rate derivatives and foreign-exchange contracts, which generally have lower volatility, are assigned smaller supervisory factors than commodities or equities. This design introduces a more realistic ranking of exposure risk across asset classes. A notable improvement under SA-CCR is its explicit recognition of legally enforceable netting sets. Exposures from offsetting trades can be aggregated and reduced within a netting agreement, which aligns the regulatory capital calculation with the true economic exposure of the portfolio. Similarly, the framework incorporates the effects of collateral by reducing the replacement cost through the current value of posted margin. This adjustment brings regulatory exposure calculations closer to the actual risk faced by the bank under daily margining and variation settlement practices. In practical terms, this change was essential: by 2015, over 80% of global OTC derivatives were collateralized under ISDA Credit Support Annexes (ISDA, 2019). Despite these improvements, SA-CCR remains a standardized formula, not a full stochastic model. Supervisory factors are fixed and updated infrequently, meaning they cannot perfectly reflect changing market volatilities or correlations across risk factors (Brigo & Vrins, 2016). In particular, wrong-way risk—where exposure increases as counterparty credit quality declines—must still be addressed through Pillar 2 supervisory adjustments rather than within the SA-CCR formula itself. Moreover, because the calibration of supervisory factors is designed to be conservative, capital requirements under SA-CCR are intentionally higher for certain asset classes compared with internal models (BCBS, 2017). This conservatism ensures stability but at the cost of precision. From a regulatory perspective, SA-CCR strikes a deliberate balance between risk sensitivity and comparability. The Basel Committee’s post-crisis studies revealed that internal model outputs for identical portfolios could differ by more than 40%, largely due to differences in simulation parameters, correlations, and data quality (EBA, 2016). By introducing SA-CCR as the universal standardized method, regulators aimed to restore consistency across banks and reduce opportunities for model arbitrage. At the same time, SA-CCR remains sufficiently risk-sensitive to reward prudent risk management practices such as collateralization and netting. The method thereby embeds micro-level incentives into a macro-prudential structure. Operationally, implementing SA-CCR has been challenging. The framework requires detailed data on notional amounts, supervisory deltas, and maturity factors for every trade. Institutions had to overhaul internal systems to calculate and aggregate exposures by product class and hedging set. This transition imposed significant compliance costs, particularly for smaller banks without advanced risk-infrastructure capabilities (EBA, 2021). Nonetheless, the result has been a more transparent and harmonized capital framework, reducing interbank disparities and facilitating cross-border supervision.

The introduction of SA-CCR also carries important economic implications. By differentiating capital treatment across asset classes and collateralization levels, it indirectly shapes banks’ incentives to trade specific products and counterparties. For example, fully collateralized or cleared derivatives now attract substantially lower capital charges than bilateral, uncollateralized exposures. This capital differentiation reinforces the post-crisis regulatory goal of promoting central clearing and reducing systemic interconnections through unsecured OTC trading (BCBS, 2017; Gregory, 2015). SA-CCR represents a major advancement in the regulatory measurement of counterparty credit risk. It successfully replaces the oversimplified and static structure of CEM with a framework that integrates exposure dynamics, collateral effects, and product-specific risk factors. Although it still lacks the full risk sensitivity of internal models and may overstate capital for some hedged or short-dated portfolios, its transparency, comparability, and policy coherence make it a cornerstone of the Basel III and IV regulatory architecture. SA-CCR thus reflects the post-crisis philosophy of prudence through simplicity—ensuring that capital standards remain robust even as market structures and risk-management practices continue to evolve.


## 3.6 Internal Model Method (IMM)

The Internal Model Method (IMM) represents the most advanced regulatory approach for measuring counterparty credit risk (CCR) under the Basel framework. It allows banks, subject to supervisory approval, to use internal Monte Carlo simulation models to estimate exposure distributions over time. Compared with standardized approaches such as the Current Exposure Method (CEM) and the Standardized Approach for Counterparty Credit Risk (SA-CCR), the IMM offers a far more risk-sensitive representation of exposure by explicitly modeling the stochastic evolution of market variables and portfolio values (BCBS, 2011). In principle, it aligns regulatory capital with the economic reality of derivatives trading, capturing netting, collateralization, and non-linear payoffs that simpler formulas cannot represent.

Under the IMM, the exposure at default (EAD) is derived from the time-weighted average of expected positive exposures, or effective expected positive exposure (EEPE), multiplied by a regulatory scaling factor. The general expression is:

EAD = α × EEPE,  

where α = 1.4 serves as a prudential multiplier to account for model risk and estimation uncertainty (BCBS, 2011). EEPE is obtained by simulating potential future portfolio values across thousands of market scenarios and averaging the expected positive exposure (EPE) over time. This structure enables the IMM to account for the dynamic nature of exposure—how it evolves as market conditions, collateral, and netting arrangements change throughout the life of the contract.

The key advantage of the IMM lies in its flexibility. By modeling risk factors such as interest rates, foreign exchange rates, and credit spreads jointly, the method can reflect correlations and non-linear sensitivities that drive real-world exposure. For example, a bank using IMM can model the effect of volatility skew on option portfolios or the impact of collateral thresholds on exposure profiles. This flexibility is especially important for institutions with complex, diversified derivatives books, where standardized approaches may overstate or understate exposures due to their simplified assumptions (Glasserman, 2003; Gregory, 2015).  Moreover, the IMM provides a consistent framework across regulatory and internal risk-management functions. Because the same Monte Carlo engine can be used to price derivatives, calculate Value-at-Risk (VaR), and estimate exposure, it ensures coherence between capital calculations and daily risk monitoring. This integration enhances transparency within the institution and improves the dialogue between risk managers and supervisors. It also supports strategic decisions such as trade pricing, collateral management, and counterparty selection by providing granular visibility into how each factor contributes to capital consumption.However, the IMM’s sophistication comes with considerable operational and supervisory challenges. Implementing an internal model requires extensive historical data, high computational capacity, and robust governance frameworks to ensure model integrity. Banks must validate their assumptions about volatility, correlation, and credit-spread dynamics, while supervisors require ongoing back-testing, benchmarking, and periodic re-approval (Mathur & Skoglund, 2013). These requirements make IMM adoption both costly and time-consuming, restricting its use primarily to large international institutions with advanced quantitative infrastructures.

One of the most persistent criticisms of the IMM is the dispersion in model outputs across banks. Studies by the European Banking Authority (EBA, 2016) and the Basel Committee have shown that, even when using similar portfolios, IMM users can report EAD values that differ by more than 30%. These discrepancies arise from differences in modeling assumptions, calibration data, and simulation techniques. For supervisors, such variability undermines the comparability of risk-weighted assets (RWAs) and weakens the credibility of the capital framework. The IMM’s reliance on internal parameters effectively transfers much of the burden of prudence from regulation to bank governance, raising concerns about transparency and consistency across jurisdictions (BCBS, 2020). Another limitation is that IMM models can be procyclical. Because they rely on recent historical data to calibrate volatility and correlation, risk estimates often decline during calm periods and spike during market stress. This behavior can amplify systemic risk, as banks simultaneously experience surges in capital requirements during crises when capital is already scarce. Regulators have responded to this concern by introducing stressed calibration periods and additional multipliers, but the issue remains a fundamental trade-off between sensitivity and stability (Green & Kenyon, 2015). Despite these challenges, the IMM has proven invaluable in improving the industry’s understanding of counterparty exposure dynamics. It provides insights into how netting and collateralization influence the shape of exposure distributions, how wrong-way risk can magnify tail outcomes, and how portfolio diversification reduces aggregate CCR. Many of these analytical tools have since influenced the design of more standardized frameworks like SA-CCR and SA-CVA, demonstrating the IMM’s enduring conceptual importance even as its regulatory role becomes more limited.

In recent years, the Basel Committee has moved toward restricting the use of internal models in favor of standardized methods to improve cross-bank comparability. Under Basel IV, the CVA-VaR model and certain IMM components have been replaced by simpler standardized equivalents, reflecting a broader shift in regulatory philosophy from micro-level precision to macro-level consistency (BCBS, 2020). Nonetheless, for the largest global dealers, the IMM remains indispensable for managing complex, bespoke portfolios and performing internal economic capital assessments.

The Internal Model Method stands as the most risk-sensitive yet operationally demanding approach to counterparty credit risk measurement. It embodies the Basel philosophy of aligning capital with actual economic exposure but also highlights the inherent tension between accuracy, transparency, and supervisory control. The lessons learned from IMM implementation—both its strengths in modeling realism and its weaknesses in comparability—have directly shaped the design of the more standardized and pragmatic frameworks that followed under Basel IV.




